{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install supabase\n",
        "!pip install --user -U nltk\n",
        "!pip install --user -U numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V9yFCCGFAfXY",
        "outputId": "df52ff04-ee6d-4bb5-8432-51962794a6db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supabase in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: gotrue<3.0.0,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from supabase) (2.10.0)\n",
            "Requirement already satisfied: httpx<0.28,>=0.26 in /usr/local/lib/python3.10/dist-packages (from supabase) (0.27.2)\n",
            "Requirement already satisfied: postgrest<0.19,>=0.18 in /usr/local/lib/python3.10/dist-packages (from supabase) (0.18.0)\n",
            "Requirement already satisfied: realtime<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from supabase) (2.0.6)\n",
            "Requirement already satisfied: storage3<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from supabase) (0.9.0)\n",
            "Requirement already satisfied: supafunc<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from gotrue<3.0.0,>=2.10.0->supabase) (2.9.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.26->supabase) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.26->supabase) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.26->supabase) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28,>=0.26->supabase) (0.14.0)\n",
            "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from postgrest<0.19,>=0.18->supabase) (2.1.0)\n",
            "Requirement already satisfied: strenum<0.5.0,>=0.4.9 in /usr/local/lib/python3.10/dist-packages (from postgrest<0.19,>=0.18->supabase) (0.4.15)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.10.10 in /usr/local/lib/python3.10/dist-packages (from realtime<3.0.0,>=2.0.0->supabase) (3.10.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from realtime<3.0.0,>=2.0.0->supabase) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from realtime<3.0.0,>=2.0.0->supabase) (4.12.2)\n",
            "Requirement already satisfied: websockets<14,>=11 in /usr/local/lib/python3.10/dist-packages (from realtime<3.0.0,>=2.0.0->supabase) (13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.10->realtime<3.0.0,>=2.0.0->supabase) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.10->realtime<3.0.0,>=2.0.0->supabase) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.10->realtime<3.0.0,>=2.0.0->supabase) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.10->realtime<3.0.0,>=2.0.0->supabase) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.10->realtime<3.0.0,>=2.0.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.10->realtime<3.0.0,>=2.0.0->supabase) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.10.10->realtime<3.0.0,>=2.0.0->supabase) (4.0.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation<3.0.0,>=2.1.0->postgrest<0.19,>=0.18->supabase) (24.1)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<0.28,>=0.26->gotrue<3.0.0,>=2.10.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.10.0->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.10.0->supabase) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<3.0.0,>=2.0.0->supabase) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28,>=0.26->supabase) (1.2.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<0.28,>=0.26->gotrue<3.0.0,>=2.10.0->supabase) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<0.28,>=0.26->gotrue<3.0.0,>=2.10.0->supabase) (4.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.10.10->realtime<3.0.0,>=2.0.0->supabase) (0.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed nltk-3.9.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.3 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
            "langchain 0.3.4 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.1.3 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.1.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.3 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "u4CC7hON_x1e"
      },
      "outputs": [],
      "source": [
        "from supabase import create_client, Client\n",
        "\n",
        "url: str = \"https://jryeokpjkidbgzscmrcj.supabase.co\"\n",
        "key: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImpyeWVva3Bqa2lkYmd6c2NtcmNqIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MjU5ODQ3NjIsImV4cCI6MjA0MTU2MDc2Mn0.nLeYIrrnbkSVqKeY7XOZkgHxDYwDcQOSVwmzrZgQrMo\"\n",
        "\n",
        "supabase: Client = create_client(url, key)\n",
        "\n",
        "rating_rubric_1_Yes = supabase.table(\"response_distinct\").select(\"text\").eq(\"rubric_1\", \"Yes\").execute()\n",
        "rubric_1_Yes = rating_rubric_1_Yes.data\n",
        "rating_rubric_1_No = supabase.table(\"response_distinct\").select(\"text\").eq(\"rubric_1\", \"No\").execute()\n",
        "rubric_1_No = rating_rubric_1_No.data\n",
        "\n",
        "rating_rubric_2_Yes = supabase.table(\"response_distinct\").select(\"text\").eq(\"rubric_2\", \"Yes\").execute()\n",
        "rubric_2_Yes = rating_rubric_2_Yes.data\n",
        "rating_rubric_2_No = supabase.table(\"response_distinct\").select(\"text\").eq(\"rubric_2\", \"No\").execute()\n",
        "rubric_2_No = rating_rubric_2_No.data\n",
        "\n",
        "rating_rubric_4_Yes = supabase.table(\"response_distinct\").select(\"text\").eq(\"rubric_4\", \"Yes\").execute()\n",
        "rubric_4_Yes = rating_rubric_4_Yes.data\n",
        "rating_rubric_4_No = supabase.table(\"response_distinct\").select(\"text\").eq(\"rubric_4\", \"No\").execute()\n",
        "rubric_4_No = rating_rubric_4_No.data\n",
        "\n",
        "rating_rubric_5_Yes = supabase.table(\"response_distinct\").select(\"text\").eq(\"rubric_5\", \"Yes\").execute()\n",
        "rubric_5_Yes = rating_rubric_5_Yes.data\n",
        "rating_rubric_5_No = supabase.table(\"response_distinct\").select(\"text\").eq(\"rubric_5\", \"No\").execute()\n",
        "rubric_5_No = rating_rubric_5_No.data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_rubric1_yes = pd.DataFrame(rubric_1_Yes)\n",
        "df_rubric1_no = pd.DataFrame(rubric_1_No)\n",
        "\n",
        "df_rubric2_yes = pd.DataFrame(rubric_2_Yes)\n",
        "df_rubric2_no = pd.DataFrame(rubric_2_No)\n",
        "\n",
        "df_rubric4_yes = pd.DataFrame(rubric_4_Yes)\n",
        "df_rubric4_no = pd.DataFrame(rubric_4_No)\n",
        "\n",
        "df_rubric5_yes = pd.DataFrame(rubric_5_Yes)\n",
        "df_rubric5_no = pd.DataFrame(rubric_5_No)\n",
        "\n"
      ],
      "metadata": {
        "id": "-0T7KMSCCRt5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "joined_r1_y = '\\n'.join(df_rubric1_yes['text'].tolist())\n",
        "tokens_r1_y = nltk.word_tokenize(joined_r1_y)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "dist_r1_y = nltk.FreqDist([w.lower() for w in tokens_r1_y])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r1_y.most_common(10))\n",
        "\n",
        "joined_r1_n = '\\n'.join(df_rubric1_no['text'].tolist())\n",
        "tokens_r1_n = nltk.word_tokenize(joined_r1_n)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "dist_r1_n = nltk.FreqDist([w.lower() for w in tokens_r1_n])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r1_n.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYU519nMBYQr",
        "outputId": "37967ad0-f93a-4754-e32e-abf50ff25280"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 58), ('the', 44), ('is', 30), ('of', 21), ('to', 18), (',', 13), ('that', 12), ('you', 12), ('and', 11), ('your', 10)]\n",
            "[('the', 187), ('.', 184), ('?', 134), ('you', 131), (',', 108), ('to', 95), ('is', 87), ('how', 85), ('that', 62), ('a', 62)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rubric 2\n",
        "joined_r2_y = '\\n'.join(df_rubric2_yes['text'].tolist())\n",
        "tokens_r2_y = nltk.word_tokenize(joined_r2_y)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "dist_r2_y = nltk.FreqDist([w.lower() for w in tokens_r2_y])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r2_y.most_common(10))\n",
        "\n",
        "joined_r2_n = '\\n'.join(df_rubric2_no['text'].tolist())\n",
        "tokens_r2_n = nltk.word_tokenize(joined_r2_n)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "dist_r2_n = nltk.FreqDist([w.lower() for w in tokens_r2_n])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r2_n.most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgP63CzqU51O",
        "outputId": "e82cf64b-af39-451a-8eeb-e3d0516140b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 168), ('.', 167), ('?', 158), ('you', 135), (',', 102), ('how', 94), ('to', 81), ('is', 79), ('of', 67), ('a', 63)]\n",
            "[('.', 126), ('the', 91), ('is', 45), ('to', 39), ('you', 37), (',', 35), ('try', 33), ('that', 31), ('a', 31), ('good', 28)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rubric 4\n",
        "joined_r4_y = '\\n'.join(df_rubric4_yes['text'].tolist())\n",
        "tokens_r4_y = nltk.word_tokenize(joined_r4_y)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "dist_r4_y = nltk.FreqDist([w.lower() for w in tokens_r4_y])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r4_y.most_common(10))\n",
        "\n",
        "joined_r4_n = '\\n'.join(df_rubric4_no['text'].tolist())\n",
        "tokens_r4_n = nltk.word_tokenize(joined_r4_n)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "dist_r4_n = nltk.FreqDist([w.lower() for w in tokens_r4_n])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r4_n.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjZyjwA3U887",
        "outputId": "d24c2af4-a0dc-4f92-d7fb-999c52603e9f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 98), ('the', 81), ('you', 58), ('!', 51), ('to', 40), ('try', 38), ('is', 37), ('a', 37), ('?', 35), ('great', 31)]\n",
            "[('.', 131), ('the', 128), ('?', 97), ('you', 80), (',', 71), ('is', 63), ('to', 59), ('how', 55), ('of', 43), ('that', 39)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rubric 5\n",
        "joined_r5_y = '\\n'.join(df_rubric5_yes['text'].tolist())\n",
        "tokens_r5_y = nltk.word_tokenize(joined_r5_y)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "dist_r5_y = nltk.FreqDist([w.lower() for w in tokens_r5_y])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r5_y.most_common(10))\n",
        "\n",
        "joined_r5_n = '\\n'.join(df_rubric5_no['text'].tolist())\n",
        "tokens_r5_n = nltk.word_tokenize(joined_r5_n)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "dist_r5_n = nltk.FreqDist([w.lower() for w in tokens_r5_n])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r5_n.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh3nZ118VAPs",
        "outputId": "b376febc-6fe4-4268-cf4f-afc0a72b65de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 164), ('.', 146), ('you', 92), (',', 89), ('?', 82), ('is', 80), ('to', 69), ('of', 53), ('we', 48), ('how', 44)]\n",
            "[('.', 66), ('the', 28), ('you', 27), ('try', 23), ('?', 23), ('your', 21), ('a', 19), ('is', 19), ('that', 18), ('good', 18)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# w.isalpha() returns True if all characters in w are alphabetic\n",
        "dist_r1_y = nltk.FreqDist([\n",
        "    w.lower() for w in tokens_r1_y\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r1_y.most_common(10))\n",
        "\n",
        "# w.isalpha() returns True if all characters in w are alphabetic\n",
        "dist_r1_n = nltk.FreqDist([\n",
        "    w.lower() for w in tokens_r1_n\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r1_n.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3PAorUaSkSk",
        "outputId": "b68834ee-b7ad-43b7-81ce-0cb2fd7fc425"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('answer', 9), ('try', 8), ('two', 7), ('good', 7), ('incorrect', 5), ('centimeters', 5), ('crayon', 5), ('sides', 4), ('value', 4), ('let', 4)]\n",
            "[('many', 45), ('try', 43), ('let', 37), ('good', 24), ('ok', 19), ('great', 18), ('need', 17), ('see', 17), ('question', 16), ('answer', 15)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rubric 2\n",
        "dist_r2_y = nltk.FreqDist([\n",
        "    w.lower() for w in tokens_r2_y\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r2_y.most_common(10))\n",
        "\n",
        "dist_r2_n = nltk.FreqDist([\n",
        "    w.lower() for w in tokens_r2_n\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r2_n.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sRzTBdKVINI",
        "outputId": "0220a935-83ec-4b32-f5b0-21d465c327ea"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('many', 49), ('let', 33), ('try', 29), ('ok', 20), ('question', 19), ('good', 18), ('great', 17), ('think', 17), ('much', 17), ('would', 15)]\n",
            "[('try', 33), ('good', 28), ('point', 18), ('answer', 13), ('two', 11), ('effort', 11), ('plus', 11), ('let', 10), ('question', 9), ('great', 8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rubric 4\n",
        "dist_r4_y = nltk.FreqDist([\n",
        "    w.lower() for w in tokens_r4_y\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r4_y.most_common(10))\n",
        "\n",
        "dist_r4_n = nltk.FreqDist([\n",
        "    w.lower() for w in tokens_r4_n\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r4_n.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gasNPzPQVYKi",
        "outputId": "fdc3d76b-c237-4dbf-e1d3-c1d36e9a4532"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('try', 38), ('great', 31), ('good', 22), ('let', 15), ('point', 11), ('question', 9), ('please', 9), ('find', 8), ('explain', 8), ('answer', 8)]\n",
            "[('many', 29), ('answer', 19), ('try', 16), ('question', 15), ('ok', 14), ('x', 13), ('good', 13), ('incorrect', 12), ('let', 12), ('see', 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rubric 5\n",
        "dist_r5_y = nltk.FreqDist([\n",
        "    w.lower() for w in tokens_r5_y\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r5_y.most_common(10))\n",
        "\n",
        "dist_r5_n = nltk.FreqDist([\n",
        "    w.lower() for w in tokens_r5_n\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(dist_r5_n.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQPL_MFNVhia",
        "outputId": "f35165fa-3184-4892-a3be-96fb04e21d41"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('many', 23), ('try', 17), ('let', 17), ('need', 15), ('question', 14), ('see', 13), ('quite', 12), ('x', 12), ('ok', 12), ('good', 11)]\n",
            "[('try', 23), ('good', 18), ('answer', 13), ('point', 12), ('effort', 9), ('let', 8), ('explain', 8), ('plus', 8), ('please', 7), ('recheck', 6)]\n"
          ]
        }
      ]
    }
  ]
}